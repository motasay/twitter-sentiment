{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 650M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import codecs, sys\n",
    "\n",
    "from word2vec.word2vecReader import Word2Vec\n",
    "from preprocessing import preprocess_tweet\n",
    "from features import get_word2vec_features, NUM_LINGUISTIC_FEATURES, get_linguistic_features\n",
    "from nn import NN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_evaluations(Y_true, Y_pred):\n",
    "    report = classification_report(Y_true, Y_pred)\n",
    "    print 'Classification report:\\n%s' % str(report)\n",
    "\n",
    "    cm = confusion_matrix(Y_true, Y_pred)\n",
    "    print 'Confusion Matrix:\\n%s' % str(cm)\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "\n",
    "def load_word2vec(path='../models/word2vec_twitter_model.bin'):\n",
    "    return Word2Vec.load_word2vec_format(path, binary=True)\n",
    "\n",
    "def load_data(vec_function, num_features, num_test_samples_per_class=500):\n",
    "    # first load the raw data\n",
    "    f = codecs.open('../data/positive-all', 'r', 'utf-8')\n",
    "    positive = {l.strip() for l in f}\n",
    "    f.close()\n",
    "\n",
    "    f = codecs.open('../data/negative-all', 'r', 'utf-8')\n",
    "    negative = {l.strip() for l in f}\n",
    "    f.close()\n",
    "\n",
    "    f = codecs.open('../data/neutral-all', 'r', 'utf-8')\n",
    "    neutral = {l.strip() for l in f}\n",
    "    f.close()\n",
    "    \n",
    "    # convert the sentences to vectors\n",
    "    positive_features = np.zeros((len(positive), num_features), dtype=np.float32)\n",
    "    negative_features = np.zeros((len(negative), num_features), dtype=np.float32)\n",
    "    neutral_features  = np.zeros((len(neutral) , num_features), dtype=np.float32)\n",
    "\n",
    "    for i, sentence in enumerate(positive):\n",
    "        sent_vec = vec_function(sentence)\n",
    "        positive_features[i,] = sent_vec\n",
    "\n",
    "    for i, sentence in enumerate(negative):\n",
    "        sent_vec = vec_function(sentence)\n",
    "        negative_features[i,] = sent_vec\n",
    "\n",
    "    for i, sentence in enumerate(neutral):\n",
    "        sent_vec = vec_function(sentence)\n",
    "        neutral_features[i,] = sent_vec\n",
    "    \n",
    "    # finally split into train/test and combine them into one big matrix\n",
    "    pos_train, pos_test = train_test_split(positive_features, test_size=num_test_samples_per_class, random_state=22)\n",
    "    neg_train, neg_test = train_test_split(negative_features, test_size=num_test_samples_per_class, random_state=22)\n",
    "    neu_train, neu_test = train_test_split(neutral_features , test_size=num_test_samples_per_class, random_state=22)\n",
    "\n",
    "    X_train = np.vstack((\n",
    "        pos_train,\n",
    "        neg_train,\n",
    "        neu_train\n",
    "    ))\n",
    "    X_test  = np.vstack((\n",
    "        pos_test,\n",
    "        neg_test,\n",
    "        neu_test\n",
    "    ))\n",
    "    Y_train = np.hstack((\n",
    "        np.ones((pos_train.shape[0]), dtype=np.float32),\n",
    "        np.ones((neg_train.shape[0]), dtype=np.float32) * -1,\n",
    "        np.zeros((neu_train.shape[0]), dtype=np.float32)\n",
    "    ))\n",
    "    Y_test = np.hstack((\n",
    "        np.ones((pos_test.shape[0]), dtype=np.float32),\n",
    "        np.ones((neg_test.shape[0]), dtype=np.float32) * -1,\n",
    "        np.zeros((neu_test.shape[0]), dtype=np.float32)\n",
    "    ))\n",
    "\n",
    "    # shuffle 'em\n",
    "    X_train, Y_train = shuffle(X_train, Y_train, random_state=111)\n",
    "    X_test , Y_test  = shuffle(X_test , Y_test , random_state=111)\n",
    "    \n",
    "    return X_train, Y_train, X_test , Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the word2vec features and the linguistic ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the word2vec model...\n",
      "Building the word2vec sentence features...\n",
      "Building the linguistic features...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(142)\n",
    "\n",
    "print 'Loading the word2vec model...'\n",
    "sys.stdout.flush()\n",
    "w2v = load_word2vec()\n",
    "\n",
    "print 'Building the word2vec sentence features...'\n",
    "sys.stdout.flush()\n",
    "\n",
    "vec_function = lambda sentence: get_word2vec_features(w2v, sentence)\n",
    "num_features = w2v.layer1_size\n",
    "w2v_X_train, w2v_Y_train, w2v_X_test , w2v_Y_test = load_data(vec_function, num_features)\n",
    "\n",
    "print 'Building the linguistic features...'\n",
    "sys.stdout.flush()\n",
    "\n",
    "vec_function = lambda sentence: get_linguistic_features(sentence)\n",
    "num_features = NUM_LINGUISTIC_FEATURES\n",
    "ling_X_train, ling_Y_train, ling_X_test , ling_Y_test = load_data(vec_function, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a Logistic Regression model on each and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a logistic regression model on the word2vec features...\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.83      0.89      0.86       500\n",
      "        0.0       0.71      0.79      0.75       500\n",
      "        1.0       0.77      0.64      0.70       500\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[444  31  25]\n",
      " [ 39 393  68]\n",
      " [ 52 129 319]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    7.2s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   35.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-5 accuracy: 0.76345 [0.76044 - 0.76646]\n",
      "Training a logistic regression model on the linguistic features...\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.65      0.75      0.70       500\n",
      "        0.0       0.54      0.52      0.53       500\n",
      "        1.0       0.63      0.54      0.58       500\n",
      "\n",
      "avg / total       0.60      0.61      0.60      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[377  72  51]\n",
      " [130 262 108]\n",
      " [ 77 154 269]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-5 accuracy: 0.62213 [0.60591 - 0.63835]\n"
     ]
    }
   ],
   "source": [
    "del w2v # don't need it anymore\n",
    "\n",
    "print 'Training a logistic regression model on the word2vec features...'\n",
    "sys.stdout.flush()\n",
    "\n",
    "w2v_lr = LogisticRegression(C=1e5, class_weight='auto', random_state=33)\n",
    "w2v_lr.fit(w2v_X_train, w2v_Y_train)\n",
    "\n",
    "predictions = w2v_lr.predict(w2v_X_test)\n",
    "print_evaluations(w2v_Y_test, predictions)\n",
    "\n",
    "# In addition, let's do CV and print out the results\n",
    "scores = cross_val_score(w2v_lr, w2v_X_train, w2v_Y_train, cv=5, verbose=1)\n",
    "print \"CV-5 accuracy: %0.5f [%0.5f - %0.5f]\" % (scores.mean(), scores.mean()-scores.std() * 2, scores.mean()+scores.std() * 2)\n",
    "sys.stdout.flush()\n",
    "\n",
    "print 'Training a logistic regression model on the linguistic features...'\n",
    "sys.stdout.flush()\n",
    "\n",
    "ling_lr = LogisticRegression(C=1e5, class_weight='auto', random_state=33)\n",
    "ling_lr.fit(ling_X_train, ling_Y_train)\n",
    "\n",
    "predictions = ling_lr.predict(ling_X_test)\n",
    "print_evaluations(ling_Y_test, predictions)\n",
    "\n",
    "# In addition, let's do CV and print out the results\n",
    "scores = cross_val_score(ling_lr, ling_X_train, ling_Y_train, cv=5, verbose=1)\n",
    "print \"CV-5 accuracy: %0.5f [%0.5f - %0.5f]\" % (scores.mean(), scores.mean()-scores.std() * 2, scores.mean()+scores.std() * 2)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's combine the outputs of the word2vec model with the linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on both train and test\n",
    "w2v_train_predictions = w2v_lr.predict(w2v_X_train)\n",
    "w2v_test_predictions  = w2v_lr.predict(w2v_X_test)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "w2v_train_predictions_binarized = mlb.fit_transform(w2v_train_predictions.reshape(-1, 1))\n",
    "w2v_test_predictions_binarized  = mlb.fit_transform(w2v_test_predictions.reshape(-1, 1))\n",
    "\n",
    "# now stack these with the ling features\n",
    "# now combine the features and train a new classifier\n",
    "X_train = np.hstack((\n",
    "    w2v_train_predictions_binarized,\n",
    "    ling_X_train\n",
    "))\n",
    "X_test = np.hstack((\n",
    "    w2v_test_predictions_binarized,\n",
    "    ling_X_test\n",
    "))\n",
    "\n",
    "# normalise to unit length\n",
    "lengths = np.linalg.norm(X_train, axis=1)\n",
    "X_train = X_train / lengths[:, None] # divides each row by the corresponding element\n",
    "lengths = np.linalg.norm(X_test, axis=1)\n",
    "X_test  = X_test / lengths[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's train a logistic regression model on the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a logistic regression model on the combined features...\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.83      0.89      0.86       500\n",
      "        0.0       0.71      0.78      0.74       500\n",
      "        1.0       0.77      0.64      0.70       500\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[444  31  25]\n",
      " [ 40 391  69]\n",
      " [ 50 130 320]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-5 accuracy: 0.77595 [0.76981 - 0.78209]\n"
     ]
    }
   ],
   "source": [
    "print 'Training a logistic regression model on the combined features...'\n",
    "sys.stdout.flush()\n",
    "lr = LogisticRegression(C=1e5, class_weight='auto', random_state=33)\n",
    "lr.fit(X_train, ling_Y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "print_evaluations(ling_Y_test, predictions)\n",
    "\n",
    "# In addition, let's do CV and print out the results\n",
    "scores = cross_val_score(lr, X_train, ling_Y_train, cv=5, verbose=1)\n",
    "print \"CV-5 accuracy: %0.5f [%0.5f - %0.5f]\" % (scores.mean(), scores.mean()-scores.std() * 2, scores.mean()+scores.std() * 2)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The precision/recall/F1 scores are almost exactly the same as the word2vec features by themselves. However, cross validation slightly favours the combined features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's checkout the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tW2V_Neg\tW2V_Neu\tW2VPos\tc(Pos)\tc(Neg)\tc(Int)\tc(Elo)\t?\t!\t!!..\t#\n",
      "Neg:\t6.04\t-2.22\t-1.52\t0.23\t3.93\t1.29\t2.29\t0.96\t-0.20\t1.70\t-2.38\t\n",
      "Neu:\t-4.39\t2.33\t-2.83\t-1.36\t-2.08\t-1.32\t-0.48\t0.86\t-1.56\t-2.31\t0.09\t\n",
      "Pos:\t-4.48\t-2.74\t2.19\t1.61\t-1.59\t0.65\t-0.55\t-2.22\t1.97\t0.61\t2.08\t\n"
     ]
    }
   ],
   "source": [
    "print '\\tW2V_Neg\\tW2V_Neu\\tW2VPos\\tc(Pos)\\tc(Neg)\\tc(Int)\\tc(Elo)\\t?\\t!\\t!!..\\t#'\n",
    "row_titles = ['Neg', 'Neu', 'Pos']\n",
    "for title, row in zip(row_titles, lr.coef_):\n",
    "    print '%s:\\t' % title,\n",
    "    for v in row:\n",
    "        print '%-.2f\\t' % v,\n",
    "    print ''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, so the word2vec features have more magnitude, but also some of the linguistic features seem to be kind of good, especially in finding the negative ones. The word2vec feature seems to be the only one that's useful in finding Neutral sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's set all the linguistic features to random numbers and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(142)\n",
    "\n",
    "random_ling_X_train = np.random.rand(ling_X_train.shape[0], ling_X_train.shape[1])\n",
    "random_ling_X_test  = np.random.rand(ling_X_test.shape[0], ling_X_test.shape[1])\n",
    "\n",
    "X_train_rand = np.hstack((\n",
    "    w2v_train_predictions_binarized,\n",
    "    random_ling_X_train\n",
    "))\n",
    "X_test_rand = np.hstack((\n",
    "    w2v_test_predictions_binarized,\n",
    "    random_ling_X_test\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train again and checkout the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.83      0.89      0.86       500\n",
      "        0.0       0.71      0.79      0.75       500\n",
      "        1.0       0.77      0.64      0.70       500\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[444  31  25]\n",
      " [ 39 393  68]\n",
      " [ 52 129 319]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-5 accuracy: 0.77595 [0.76981 - 0.78209]\n",
      "\n",
      "Coefficients:\n",
      "\tW2V_Neg\tW2V_Neu\tW2VPos\tRand1\tRand2\tRand3\tRand4\tRand5\tRand6\tRand7\tRand8\n",
      "Neg:\t2.49\t-1.90\t-1.58\t0.12\t-0.04\t0.05\t0.01\t0.08\t-0.03\t-0.02\t0.03\t\n",
      "Neu:\t-1.67\t1.72\t-0.79\t0.03\t0.04\t-0.06\t-0.08\t-0.02\t0.11\t-0.03\t0.08\t\n",
      "Pos:\t-1.82\t-0.63\t1.82\t-0.11\t-0.01\t0.02\t0.08\t-0.04\t-0.10\t0.06\t-0.11\t\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1e5, class_weight='auto', random_state=33)\n",
    "lr.fit(X_train_rand, ling_Y_train)\n",
    "\n",
    "predictions = lr.predict(X_test_rand)\n",
    "print_evaluations(ling_Y_test, predictions)\n",
    "\n",
    "# In addition, let's do CV and print out the results\n",
    "scores = cross_val_score(lr, X_train, ling_Y_train, cv=5, verbose=1)\n",
    "print \"CV-5 accuracy: %0.5f [%0.5f - %0.5f]\" % (scores.mean(), scores.mean()-scores.std() * 2, scores.mean()+scores.std() * 2)\n",
    "\n",
    "print '\\nCoefficients:'\n",
    "print '\\tW2V_Neg\\tW2V_Neu\\tW2VPos\\tRand1\\tRand2\\tRand3\\tRand4\\tRand5\\tRand6\\tRand7\\tRand8'\n",
    "row_titles = ['Neg', 'Neu', 'Pos']\n",
    "for title, row in zip(row_titles, lr.coef_):\n",
    "    print '%s:\\t' % title,\n",
    "    for v in row:\n",
    "        print '%-.2f\\t' % v,\n",
    "    print ''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same numbers as well, which suggests that the linguistic features aren't really affecting the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final experiment: Let's train on the top 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.83      0.89      0.86       500\n",
      "        0.0       0.71      0.78      0.74       500\n",
      "        1.0       0.77      0.64      0.70       500\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[444  31  25]\n",
      " [ 40 388  72]\n",
      " [ 49 129 322]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-5 accuracy: 0.77595 [0.76866 - 0.78324]\n",
      "\n",
      "Coefficients:\n",
      "\tF1\tF2\tF3\tF4\tF5\tF6\tF7\\F8\n",
      "Neg:\t4.22\t-4.31\t-3.37\t-0.74\t2.96\t0.38\t-0.54\t-2.72\t\n",
      "Neu:\t-4.00\t2.85\t-2.47\t-1.15\t-1.90\t0.96\t-1.71\t0.19\t\n",
      "Pos:\t-3.96\t-2.21\t2.77\t1.85\t-1.35\t-2.07\t2.21\t2.09\t\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "feature_selector = SelectKBest(chi2, k=8)\n",
    "feature_selector.fit(X_train, ling_Y_train)\n",
    "\n",
    "new_X_train = feature_selector.transform(X_train)\n",
    "new_X_test  = feature_selector.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(C=1e5, class_weight='auto', random_state=33)\n",
    "lr.fit(new_X_train, ling_Y_train)\n",
    "\n",
    "predictions = lr.predict(new_X_test)\n",
    "print_evaluations(ling_Y_test, predictions)\n",
    "\n",
    "# In addition, we'll do CV validation and print out the results\n",
    "scores = cross_val_score(lr, new_X_train, ling_Y_train, cv=5, verbose=1)\n",
    "print \"CV-5 accuracy: %0.5f [%0.5f - %0.5f]\" % (scores.mean(), scores.mean()-scores.std() * 2, scores.mean()+scores.std() * 2)\n",
    "\n",
    "print '\\nCoefficients:'\n",
    "print '\\tF1\\tF2\\tF3\\tF4\\tF5\\tF6\\tF7\\tF8'\n",
    "row_titles = ['Neg', 'Neu', 'Pos']\n",
    "for title, row in zip(row_titles, lr.coef_):\n",
    "    print '%s:\\t' % title,\n",
    "    for v in row:\n",
    "        print '%-.2f\\t' % v,\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nothing seems to be able to beat the word2vec features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
